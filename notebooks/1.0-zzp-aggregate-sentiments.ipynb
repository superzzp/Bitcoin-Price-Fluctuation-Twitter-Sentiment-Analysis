{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments = pd.read_csv('../data/interim/bitcoin_all_tweets_sentiments_merged_20210101-20210930.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "#1107812/1107811\n",
    "tweets_sentiments = pd.read_csv('../data/interim/bitcoin_all_tweets_sentiments_20210101-20210930.csv', index_col = 0)\n",
    "tweets_sentiments_next = pd.read_csv('../data/interim/bitcoin_all_tweets_sentiments_20210201-20210531.csv', index_col = 0)\n",
    "tweets_sentiments_next = tweets_sentiments_next[1107811:]\n",
    "tweets_sentiments_full = pd.concat([tweets_sentiments, tweets_sentiments_next])\n",
    "\n",
    "filename = '../data/interim/bitcoin_all_tweets_sentiments_merged_20210101-20210930.csv'\n",
    "tweets_sentiments_full.to_csv(filename, mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments['dateTime']= pd.to_datetime(tweets_sentiments['dateTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments = tweets_sentiments[['id', 'dateTime', 'prediction']]\n",
    "\n",
    "tweets_sentiments.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_mins_span = tweets_sentiments.groupby('prediction').resample('30min', on='dateTime').count()\n",
    "\n",
    "one_h_span = tweets_sentiments.groupby('prediction').resample('1H', on='dateTime').count()\n",
    "\n",
    "two_h_span = tweets_sentiments.groupby('prediction').resample('2H', on='dateTime').count()\n",
    "\n",
    "daily_span = tweets_sentiments.groupby('prediction').resample('D', on='dateTime').count()\n",
    "\n",
    "fifteen_mins_span = tweets_sentiments.groupby('prediction').resample('15min', on='dateTime').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_mins_span = thirty_mins_span.unstack('prediction', fill_value=0)\n",
    "thirty_mins_span = thirty_mins_span[('id')]\n",
    "\n",
    "one_h_span = one_h_span.unstack('prediction', fill_value=0)\n",
    "one_h_span = one_h_span[('id')]\n",
    "\n",
    "two_h_span = two_h_span.unstack('prediction', fill_value=0)\n",
    "two_h_span = two_h_span[('id')]\n",
    "\n",
    "daily_span = daily_span.unstack('prediction', fill_value=0)\n",
    "daily_span = daily_span[('id')]\n",
    "\n",
    "fifteen_mins_span = fifteen_mins_span.unstack('prediction', fill_value=0)\n",
    "fifteen_mins_span = fifteen_mins_span[('id')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_h_span.index.name = None\n",
    "two_h_span.columns.name = 'index'\n",
    "\n",
    "#two_h_span.loc['2021-09-10':'2021-09-30']\n",
    "daily_span.index.name = None\n",
    "daily_span.columns.name = 'index'\n",
    "\n",
    "#daily_span.loc['2021-09-10':'2021-09-30']\n",
    "\n",
    "one_h_span.index.name = None\n",
    "one_h_span.columns.name = 'index'\n",
    "\n",
    "thirty_mins_span.index.name = None\n",
    "thirty_mins_span.columns.name = 'index'\n",
    "\n",
    "\n",
    "fifteen_mins_span.index.name = None\n",
    "fifteen_mins_span.columns.name = 'index'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percentage(df):\n",
    "    df['total'] = df['negative'] + df['neutral'] + df['positive']\n",
    "    df['pos_percent'] = df['positive'] / df['total'] * 100 \n",
    "    df['neu_percent'] = df['neutral'] / df['total'] * 100\n",
    "    df['neg_percent'] = df['negative'] / df['total'] * 100\n",
    "\n",
    "calc_percentage(thirty_mins_span)\n",
    "calc_percentage(one_h_span)\n",
    "calc_percentage(two_h_span)\n",
    "calc_percentage(daily_span)\n",
    "calc_percentage(fifteen_mins_span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = daily_span\n",
    "\n",
    "# Define the upper limit, lower limit, interval of Y axis and colors\n",
    "y_LL = 0\n",
    "y_UL = int(plot.iloc[:, 1:].max().max()*1.1)\n",
    "#y_UL = 50000\n",
    "y_interval = 4000\n",
    "mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange']    \n",
    "\n",
    "# Draw Plot and Annotate\n",
    "fig, ax = plt.subplots(1,1,figsize=(16, 9), dpi= 80)    \n",
    "\n",
    "columns = plot.columns[:3]  \n",
    "for i, column in enumerate(columns):    \n",
    "    valt = plot[column].values\n",
    "    plt.plot(plot.index.values, plot[column].values, lw=1.5, color=mycolors[i])    \n",
    "    plt.text(plot.shape[0]+1, plot[column].values[-1], column, fontsize=14, color=mycolors[i])\n",
    "\n",
    "# Draw Tick lines  \n",
    "for y in range(y_LL, y_UL, y_interval):    \n",
    "    plt.hlines(y, xmin=0, xmax=71, colors='black', alpha=0.3, linestyles=\"--\", lw=0.5)\n",
    "\n",
    "# Decorations    \n",
    "plt.tick_params(axis=\"both\", which=\"both\", bottom=False, top=False,    \n",
    "                labelbottom=True, left=False, right=False, labelleft=True)        \n",
    "\n",
    "# Lighten borders\n",
    "plt.gca().spines[\"top\"].set_alpha(.3)\n",
    "plt.gca().spines[\"bottom\"].set_alpha(.3)\n",
    "plt.gca().spines[\"right\"].set_alpha(.3)\n",
    "plt.gca().spines[\"left\"].set_alpha(.3)\n",
    "\n",
    "plt.title('a sample title', fontsize=22)\n",
    "plt.yticks(range(y_LL, y_UL, y_interval), [str(y) for y in range(y_LL, y_UL, y_interval)], fontsize=12)    \n",
    "plt.xticks(range(0, plot.shape[0], 24), plot.index.values[::24], horizontalalignment='left', fontsize=12)    \n",
    "\n",
    "\n",
    "plt.ylim(y_LL, y_UL)    \n",
    "plt.xlim(-2, 80)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = daily_span\n",
    "\n",
    "x = plot.datetime.values\n",
    "\n",
    "pos = plot['pos_percent']\n",
    "neu = plot['neu_percent']\n",
    "neg = plot['neg_percent']\n",
    "\n",
    "mycolors = ['tab:red', 'tab:blue', 'tab:green']    \n",
    "figure(figsize = (15, 4), dpi = 80)\n",
    "plt.title('Tweets\\' Daily Sentiments (Percentage)', loc='center')\n",
    "plt.plot(x,pos, color=mycolors[2], label=\"positive\")\n",
    "plt.plot(x,neu, color=mycolors[1], label=\"neutral\")\n",
    "plt.plot(x,neg, color=mycolors[0], label=\"negative\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_mins_span = thirty_mins_span.reset_index().rename(columns={'index':'datetime'})\n",
    "one_h_span = one_h_span.reset_index().rename(columns={'index':'datetime'})\n",
    "two_h_span = two_h_span.reset_index().rename(columns={'index':'datetime'})\n",
    "daily_span = daily_span.reset_index().rename(columns={'index':'datetime'})\n",
    "fifteen_mins_span = fifteen_mins_span.reset_index().rename(columns={'index':'datetime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices = pd.read_csv('../data/external/Bitstamp_BTCUSD_2021_minute_final.csv', header = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd = bitcoin_prices[['date', 'open', 'Volume BTC']]\n",
    "bpd['date'] = pd.to_datetime(bpd[\"date\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce', utc=True)\n",
    "#bpd['Date'] = bpd['SPdateTime'].dt.strftime('%Y-%m-%d')\n",
    "bpd = bpd.set_index('date')\n",
    "#bpd.reset_index()\n",
    "\n",
    "bpd.info()\n",
    "bpd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def fetch_prices(df, span_mins):\n",
    "    c_dts = df.datetime\n",
    "    dts_1 = c_dts + datetime.timedelta(minutes=span_mins)\n",
    "    dts_2 = c_dts + datetime.timedelta(minutes=(span_mins*2))\n",
    "    dts_3 = c_dts + datetime.timedelta(minutes=(span_mins*3))\n",
    "    dts_4 = c_dts + datetime.timedelta(minutes=(span_mins*4))\n",
    "\n",
    "    df['cp'] = bpd.loc[c_dts].reset_index()['open']\n",
    "    df['1spanp'] = bpd.loc[dts_1].reset_index()['open']\n",
    "    df['2spanp'] = bpd.loc[dts_2].reset_index()['open']\n",
    "    df['3spanp'] = bpd.loc[dts_3].reset_index()['open']\n",
    "    df['4spanp'] = bpd.loc[dts_4].reset_index()['open']\n",
    "\n",
    "    df['c_p_change'] = (df['1spanp'] - df['cp']) / df['cp']\n",
    "    df['shift1_p_change'] = (df['2spanp'] - df['1spanp']) / df['1spanp']\n",
    "    df['c_plus_shift1_p_change'] = (df['2spanp'] - df['cp']) / df['cp']\n",
    "\n",
    "fetch_prices(thirty_mins_span, 30)\n",
    "fetch_prices(one_h_span, 60)\n",
    "fetch_prices(two_h_span, 120)\n",
    "fetch_prices(fifteen_mins_span, 15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_trade_vol(df, span_mins):\n",
    "    c_dts = df.datetime\n",
    "    vols = bpd.resample(str(span_mins) + 'min').sum()['Volume BTC']\n",
    "    df['vol_btc'] = vols.loc[c_dts].reset_index()['Volume BTC']\n",
    "    \n",
    "\n",
    "fetch_trade_vol(thirty_mins_span, 30)\n",
    "fetch_trade_vol(one_h_span, 60)\n",
    "fetch_trade_vol(two_h_span, 120)\n",
    "fetch_trade_vol(fifteen_mins_span, 15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_btc_vol_change (df):    \n",
    "    #vol_diff = df[['vol_btc']].diff(periods = 2)\n",
    "    vol_perc_diff = df[['vol_btc']].pct_change(periods = 2)\n",
    "    vol_perc_diff.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    vol_perc_difs = vol_perc_diff.shift(-1)\n",
    "    vol_perc_difs.rename(columns={'vol_btc': 'vol_btc_change'}, inplace=True)\n",
    "    return df.join(vol_perc_difs)\n",
    "\n",
    "thirty_mins_span = get_df_with_btc_vol_change(thirty_mins_span)\n",
    "one_h_span = get_df_with_btc_vol_change(one_h_span)\n",
    "two_h_span = get_df_with_btc_vol_change(two_h_span)\n",
    "fifteen_mins_span = get_df_with_btc_vol_change(fifteen_mins_span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_post_sentiment_change (df):    \n",
    "    sentiment_diff = df[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 1)\n",
    "    sent_difs = sentiment_diff.shift(-1)\n",
    "    sent_difs.rename(columns={'pos_percent': 'pos_post_change', 'neu_percent': 'neu_post_change', 'neg_percent':'neg_post_change'}, inplace=True)\n",
    "    return df.join(sent_difs)\n",
    "\n",
    "thirty_mins_span = get_df_with_post_sentiment_change(thirty_mins_span)\n",
    "one_h_span = get_df_with_post_sentiment_change(one_h_span)\n",
    "two_h_span = get_df_with_post_sentiment_change(two_h_span)\n",
    "fifteen_mins_span = get_df_with_post_sentiment_change(fifteen_mins_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thirty_diff = thirty_mins_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 2)\n",
    "thirty_difs = thirty_diff.shift(-1)\n",
    "thirty_difs.rename(columns={'pos_percent': 'pos_change', 'neu_percent': 'neu_change', 'neg_percent': 'neg_change'}, inplace=True)\n",
    "thirty_mins_span_j = thirty_mins_span.join(thirty_difs)\n",
    "\n",
    "\n",
    "one_h_diff = one_h_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 2)\n",
    "one_h_difs = one_h_diff.shift(-1)\n",
    "one_h_difs.rename(columns={'pos_percent': 'pos_change', 'neu_percent': 'neu_change', 'neg_percent': 'neg_change'}, inplace=True)\n",
    "one_h_span_j = one_h_span.join(one_h_difs)\n",
    "\n",
    "two_h_diff = two_h_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 2)\n",
    "two_h_difs = two_h_diff.shift(-1)\n",
    "two_h_difs.rename(columns={'pos_percent': 'pos_change', 'neu_percent': 'neu_change', 'neg_percent': 'neg_change'}, inplace=True)\n",
    "two_h_span_j = two_h_span.join(two_h_difs)\n",
    "\n",
    "fifteen_mins_diff = fifteen_mins_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 2)\n",
    "fifteen_mins_difs = fifteen_mins_diff.shift(-1)\n",
    "fifteen_mins_difs.rename(columns={'pos_percent': 'pos_change', 'neu_percent': 'neu_change', 'neg_percent': 'neg_change'}, inplace=True)\n",
    "fifteen_mins_span_j = fifteen_mins_span.join(fifteen_mins_difs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_pdiff = thirty_mins_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 1)\n",
    "thirty_pdiff.rename(columns={'pos_percent': 'pos_pre_change', 'neu_percent': 'neu_pre_change', 'neg_percent': 'neg_pre_change'}, inplace=True)\n",
    "thirty_mins_span_j = thirty_mins_span_j.join(thirty_pdiff)\n",
    "\n",
    "one_h_pdiff = one_h_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 1)\n",
    "one_h_pdiff.rename(columns={'pos_percent': 'pos_pre_change', 'neu_percent': 'neu_pre_change', 'neg_percent': 'neg_pre_change'}, inplace=True)\n",
    "one_h_span_j = one_h_span_j.join(one_h_pdiff)\n",
    "\n",
    "two_h_pdiff = two_h_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 1)\n",
    "two_h_pdiff.rename(columns={'pos_percent': 'pos_pre_change', 'neu_percent': 'neu_pre_change', 'neg_percent': 'neg_pre_change'}, inplace=True)\n",
    "two_h_span_j = two_h_span_j.join(two_h_pdiff)\n",
    "\n",
    "fifteen_mins_pdiff = fifteen_mins_span[['pos_percent', 'neu_percent', 'neg_percent']].diff(periods = 1)\n",
    "fifteen_mins_pdiff.rename(columns={'pos_percent': 'pos_pre_change', 'neu_percent': 'neu_pre_change', 'neg_percent': 'neg_pre_change'}, inplace=True)\n",
    "fifteen_mins_span_j = fifteen_mins_span_j.join(fifteen_mins_pdiff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_mins_span_j[['pos_percent', 'pos_pre_change', 'pos_change', 'pos_post_change', 'vol_btc_change' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_shift_target(df):\n",
    "    df['curr_up'] = np.where(df['1spanp'] >  df['cp'], 1, 0)\n",
    "    df['shift1_up'] = np.where(df['2spanp'] >  df['1spanp'], 1, 0)\n",
    "    \n",
    "    df['shift2_up'] = np.where(df['3spanp'] >  df['2spanp'], 1, 0)\n",
    "    df['shift3_up'] = np.where(df['4spanp'] >  df['3spanp'], 1, 0)\n",
    "    df['2_up'] = np.where(df['2spanp'] >  df['cp'], 1, 0)\n",
    "    df['3_up'] = np.where(df['3spanp'] >  df['cp'], 1, 0)\n",
    "    df['4_up'] = np.where(df['4spanp'] > df['cp'], 1, 0)\n",
    "    df['future2_up'] = np.where(df['3spanp'] >  df['1spanp'], 1, 0)\n",
    "    df['future3_up'] = np.where(df['4spanp'] >  df['1spanp'], 1, 0)\n",
    "\n",
    "assign_shift_target(thirty_mins_span_j)\n",
    "assign_shift_target(one_h_span_j)\n",
    "assign_shift_target(two_h_span_j)\n",
    "assign_shift_target(fifteen_mins_span_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_h_span_j[['pos_percent', 'neu_percent', 'neg_percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "def filter_time(df):\n",
    "    #pacific = pytz.timezone('US/Pacific')\n",
    "    #pacific_dt = df.set_index('datetime').tz_convert(pacific)\n",
    "    return df#[(pacific_dt.index.hour < 0) | (pacific_dt.index.hour > 3)]\n",
    "\n",
    "def filter_columns(df):\n",
    "    return df[['datetime', 'total', 'pos_percent', 'neu_percent', 'neg_percent', 'vol_btc', 'vol_btc_change', 'pos_change', 'neu_change', 'neg_change', \n",
    "    'pos_pre_change', 'neu_pre_change', 'neg_pre_change', 'curr_up', \n",
    "    'pos_post_change', 'neu_post_change', 'neg_post_change',\n",
    "    'c_plus_shift1_p_change', 'shift1_p_change', 'c_p_change',\n",
    "    'shift1_up', 'shift2_up', 'shift3_up', \"2_up\", \"3_up\", \"4_up\", \"future2_up\", \"future3_up\"]]\n",
    "\n",
    "thirty_mins_span_final = filter_time(filter_columns(thirty_mins_span_j))\n",
    "one_h_span_final = filter_time(filter_columns(one_h_span_j))\n",
    "two_h_span_final = filter_time(filter_columns(two_h_span_j))\n",
    "fifteen_mins_span_final = filter_time(filter_columns(fifteen_mins_span_j))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thirty_mins_span_final.shape\n",
    "#one_h_span_final.shape\n",
    "#two_h_span.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thirty_train_df = thirty_mins_span_final[:math.floor(thirty_mins_span_final.shape[0] * 0.65)]\n",
    "thirty_test_df = thirty_mins_span_final[math.floor(thirty_mins_span_final.shape[0] * 0.65):]\n",
    "\n",
    "one_h_train_df = one_h_span_final[:math.floor(one_h_span_final.shape[0] * 0.65)]\n",
    "one_h_test_df = one_h_span_final[math.floor(one_h_span_final.shape[0] * 0.65):]\n",
    "\n",
    "two_h_train_df = two_h_span_final[:math.floor(two_h_span_final.shape[0] * 0.65)]\n",
    "two_h_test_df = two_h_span_final[math.floor(two_h_span_final.shape[0] * 0.65):]\n",
    "\n",
    "fifteen_mins_train_df = fifteen_mins_span_final[:math.floor(fifteen_mins_span_final.shape[0] * 0.65)]\n",
    "fifteen_mins_test_df = fifteen_mins_span_final[math.floor(fifteen_mins_span_final.shape[0] * 0.65):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = daily_span\n",
    "\n",
    "x = plot.datetime.values\n",
    "pos_train = thirty_train_df['pos_percent']\n",
    "neu_train = thirty_train_df['neu_percent']\n",
    "neg_train = thirty_train_df['neg_percent']\n",
    "#mycolors = ['tab:red', 'tab:blue', 'tab:green']    \n",
    "figure(figsize = (15, 4), dpi = 80)\n",
    "#plt.title('Tweets\\' Daily Sentiments (Percentage)', loc='center')\n",
    "#plt.plot(x,pos, color=mycolors[2], label=\"positive\")\n",
    "#plt.plot(x,neu, color=mycolors[1], label=\"neutral\")\n",
    "#plt.plot(x,neg, color=mycolors[0], label=\"negative\")\n",
    "pos_test = thirty_test_df['pos_percent']\n",
    "neu_test = thirty_test_df['neu_percent']\n",
    "neg_test = thirty_test_df['neg_percent']\n",
    "\n",
    "plt.plot(pos_train, \"r\", label=\"train\")\n",
    "plt.plot(neu_train, \"r\")\n",
    "plt.plot(neg_train, \"r\")\n",
    "plt.plot(pos_test, \"b\", label=\"test\")\n",
    "plt.plot(neu_test, \"b\")\n",
    "plt.plot(neg_test, \"b\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"pos_pre_change\",\n",
    "    \"neu_pre_change\",\n",
    "    \"neg_pre_change\",\n",
    "    \"pos_change\",\n",
    "    \"neu_change\",\n",
    "    \"neg_change\",\n",
    "    \"neu_post_change\",\n",
    "    \"pos_post_change\",\n",
    "    \"neg_post_change\",\n",
    "    \"pos_percent\",\n",
    "    \"neu_percent\",\n",
    "    \"neg_percent\",\n",
    "    \"total\",\n",
    "    \"vol_btc_change\",\n",
    "    \"c_plus_shift1_p_change\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \n",
    "]\n",
    "drop_features = [\n",
    "    #targets\n",
    "    \"shift2_up\",\n",
    "    \"shift3_up\",\n",
    "    \"2_up\",\n",
    "    \"3_up\",\n",
    "    \"4_up\",\n",
    "    \"future2_up\",\n",
    "    \"future3_up\",\n",
    "    #\"total\",\n",
    "    #unused features\n",
    "    \"curr_up\",\n",
    "    \"shift1_up\",\n",
    "    \"datetime\",\n",
    "    \"vol_btc\",\n",
    "    #\"vol_btc_change\",\n",
    "   \"c_p_change\",\n",
    "   \"shift1_p_change\",\n",
    "]\n",
    "\n",
    "selected_train_df = one_h_train_df\n",
    "selected_test_df = one_h_test_df\n",
    "prediction_target = \"shift2_up\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    '''categorical_transformer = make_pipeline(\n",
    "        #SimpleImputer(strategy=\"constant\", fill_value=\"?\"),\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "    )'''\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        #(categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    '''ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names()\n",
    "        .tolist()\n",
    "    )'''\n",
    "    #new_columns = numeric_features + ohe_feature_names\n",
    "    new_columns = numeric_features\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[target]\n",
    "    y_test = test_df[target]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    selected_train_df,\n",
    "    selected_test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    prediction_target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.5f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.5f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"logisticregression\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30mins, 0930-0720, train 0.55, test 0.53, 1 shift\n",
    "score_lr_print_coeff(preprocessor, selected_train_df, y_train, selected_test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the fraction of cases that are fraudulent out of those that our classifier labelled as positive. The problem with a low precision is that our investigators will spend a lot of time investigating claims that are actually legitimate.\n",
    "\n",
    "Recall is the fraction of fraudulent cases our classifier finds. The problem with a low recall is that we would be paying out on a lot of undetected fraudulent claims.\n",
    "\n",
    "If we want a higher precision, so that if our classifier predicted going up, it is more likely to go up, but we detected less proportion of the cases that are going up. (less trading opportunity, higher accuracy)\n",
    "\n",
    "If we want a higher recall, then among our prediction that the bitcoin is going up, its less likely to go up, but we detected more proportion of the cases that are going up. (more trading opportunity, less accuracy)\n",
    "\n",
    "Therefore, we care more about precision? \n",
    "\n",
    "# 1h\n",
    "\n",
    "Observations:\n",
    "The recall of sementic models generally have higher recall compared to \n",
    "baseline priced models\n",
    "\n",
    "<strong>Baseline</strong>\n",
    "shift1_p_change\t-0.039855\n",
    "c_p_change\t-0.144066\n",
    "\n",
    "Train score: 0.53594\n",
    "Test score: 0.52188 (0.521878\t0.542045\t0.491753\t0.515676)\n",
    "\n",
    "c_plus_shift1_p_change\t-0.128164\n",
    "Train score: 0.53019\n",
    "Test score: 0.52668 (0.526681\t0.548200\t0.486598\t0.515565)\n",
    "\n",
    "\n",
    "Exp1\n",
    "neu_change\t0.166109\n",
    "neg_pre_change\t0.142978\n",
    "neg_post_change\t0.127850\n",
    "neu_percent\t0.030957\n",
    "pos_pre_change\t0.020187\n",
    "neg_percent\t0.020143\n",
    "pos_post_change\t0.018853\n",
    "total\t0.005838\n",
    "pos_percent\t-0.037233\n",
    "pos_change\t-0.064472\n",
    "vol_btc_change\t-0.083970\n",
    "neu_post_change\t-0.102580\n",
    "neu_pre_change\t-0.124964\n",
    "neg_change\t-0.152797\n",
    "\n",
    "Train score: 0.51265\n",
    "Test score: 0.53095 (0.530950\t0.544828\t0.570103\t0.557179)\n",
    "\n",
    "Exp2: exp1 + price factor \"c_plus_shift1_p_change\"\n",
    "Train score: 0.52703\n",
    "Test score: 0.52455 (0.5\t0.524546\t0.539778\t0.552577\t0.546103)\n",
    "\n",
    "# 30mins\n",
    "Observations: \n",
    "1. for 30 mins prices factors have a stronger influence on future price direction compared to 1h\n",
    "2. Does it mean including prices as a factor for 30 mins' prediction will have a stronger impact? YES!\n",
    "3. Including prices + sentiments factors increases all evaluation factors especially recall, compared to baseline\n",
    "4. Including prices decrease the chance of overfitting the model\n",
    "\n",
    "Baseline:\n",
    "c_plus_shift1_p_change\t-0.069482 (0.521751\t0.528804\t0.735113\t0.615120)\n",
    "\n",
    "c_p_change\t-0.001419\n",
    "shift1_p_change\t-0.100080\n",
    "Train score: 0.52206\n",
    "Test score: 0.52389 (0.523886\t0.532905\t0.681725\t0.598198)\n",
    "\n",
    "Exp1\n",
    "pos_change\t0.168012\n",
    "neg_pre_change\t0.129002\n",
    "neg_post_change\t0.113397\n",
    "neu_post_change\t0.089122\n",
    "vol_btc_change\t0.079499\n",
    "neu_pre_change\t0.077909\n",
    "neu_percent\t0.030173\n",
    "c_p_change\t0.000646\n",
    "total\t-0.011629\n",
    "pos_percent\t-0.012835\n",
    "neg_percent\t-0.016781\n",
    "shift1_p_change\t-0.099523\n",
    "neu_change\t-0.099739\n",
    "neg_change\t-0.134134\n",
    "pos_pre_change\t-0.142063\n",
    "pos_post_change\t-0.155439\n",
    "Train score: 0.52595\n",
    "Test score: 0.52148 (0.521484\t0.525637\t0.815708\t0.639308)\n",
    "\n",
    "exp2\n",
    "pos_change\t0.173532\n",
    "neg_pre_change\t0.127890\n",
    "neg_post_change\t0.118353\n",
    "neu_post_change\t0.091813\n",
    "neu_pre_change\t0.080184\n",
    "vol_btc_change\t0.076809\n",
    "neu_percent\t0.030805\n",
    "total\t-0.010345\n",
    "pos_percent\t-0.012031\n",
    "neg_percent\t-0.018764\n",
    "c_plus_shift1_p_change\t-0.068613\n",
    "neu_change\t-0.102020\n",
    "neg_change\t-0.140124\n",
    "pos_pre_change\t-0.143314\n",
    "pos_post_change\t-0.161050\n",
    "\n",
    "Train score: 0.51790\n",
    "Test score: 0.52469 (0.5\t0.524686\t0.526021\t0.866530\t0.654644)\n",
    "\n",
    "Exp3 (exp1/2 - price factors)\n",
    "Train score: 0.51603\n",
    "Test score: 0.51668 (0.516680\t0.519724\t0.926591\t0.665929)\n",
    "\n",
    "\n",
    "# 2hs\n",
    "\n",
    "c_plus_shift1_p_change\t-0.181843\n",
    "\n",
    "Train score: 0.54917\n",
    "Test score: 0.53042 (0.530416\t0.537879\t0.591667\t0.563492)\n",
    "\n",
    "\n",
    "neg_post_change\t0.079504\n",
    "neu_pre_change\t0.069734\n",
    "neg_percent\t0.057452\n",
    "pos_percent\t0.008676\n",
    "neg_change\t0.005855\n",
    "neu_change\t0.004623\n",
    "pos_change\t0.004318\n",
    "neg_pre_change\t-0.005493\n",
    "pos_post_change\t-0.021715\n",
    "neu_post_change\t-0.024930\n",
    "total\t-0.036585\n",
    "vol_btc_change\t-0.039507\n",
    "pos_pre_change\t-0.059234\n",
    "neu_percent\t-0.061897\n",
    "\n",
    "Train score: 0.52214\n",
    "Test score: 0.50800 (0.5\t0.508004\t0.529052\t0.360417\t0.428748) (0.4 0.512273\t0.512326\t0.995833\t0.676575)\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.02)\n",
    "lr_pipe1 = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "lr_pipe1.fit(selected_train_df, y_train)\n",
    "\n",
    "pr_dict = {\"threshold\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1 score\": []}\n",
    "for threshold in thresholds:\n",
    "    preds = lr_pipe1.predict_proba(selected_test_df)[:, 1] > threshold\n",
    "    pr_dict[\"threshold\"].append(threshold)\n",
    "    pr_dict[\"accuracy\"].append(accuracy_score(y_test, preds))\n",
    "    pr_dict[\"precision\"].append(precision_score(y_test, preds))\n",
    "    pr_dict[\"recall\"].append(recall_score(y_test, preds))\n",
    "    pr_dict[\"f1 score\"].append(f1_score(y_test, preds))\n",
    "\n",
    "pd.DataFrame(pr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(\n",
    "    lr_pipe1,\n",
    "    selected_train_df,\n",
    "    y_train,\n",
    "    display_labels=[\"Down\", \"Up\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, lr_pipe1.predict_proba(selected_test_df)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_test, lr_pipe1.predict(selected_test_df)),\n",
    "    recall_score(y_test, lr_pipe1.predict(selected_test_df)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
